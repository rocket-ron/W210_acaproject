{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the JSON data to S3\n",
    "\n",
    "To make it easier to deal with the re-loading of data and indexes we decided that it would be a good idea to download the JSON PUF data and put it in S3. That removes repeated hits on the servers and speeds up the reload process when we need to do so.\n",
    "\n",
    "Unfortunately there isn't a way to stream data from the URL to S3; it has to be downloaded to the local file system first and then it can be uploaded to S3. The new `boto3` Python module makes large file uploading much easier in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "from requests.exceptions import SSLError\n",
    "from urlparse import urlparse\n",
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download to a local file\n",
    "def download_file(url):\n",
    "    h = haslib.md5(item['url']).hexdigest()\n",
    "    local_file = h + '.tmp'\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(local_file, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024*64):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return local_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upload to S3 bucket\n",
    "def xfer_to_S3(file_name, bucket, key):\n",
    "    client = boto3.client('s3', 'us-west-1')\n",
    "    transfer = S3Transfer(client)\n",
    "    transfer.upload_file(file_name, bucket, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download to a local file and then transfer to S3\n",
    "# using the hashed URL as the S3 key\n",
    "def process_url(_url, bucket_name, prefix):\n",
    "    print \"Processing {0}\".format(_url)\n",
    "    hashed_url = haslib.md5(item['url']).hexdigest()\n",
    "    f = download_file(_url)\n",
    "    xfer_to_S3(f, bucket_name, prefix + str(hashed_url))\n",
    "    # os.remove(f)  \n",
    "    return hashed_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All URLs from the Machine Readable PUF\n",
    "\n",
    "This code takes the Machine Readable PUF CSV file and walks it to get every URL pointed to by the PUF file and place it in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_filename = 'machine-readable-url-puf.csv'\n",
    "\n",
    "url_list = []\n",
    "with open(csv_filename, 'r') as urlfile:\n",
    "    urls = csv.DictReader(urlfile)\n",
    "    for row in urls:\n",
    "        _url = row['URL Submitted']\n",
    "        \n",
    "        if urlparse(_url).scheme:\n",
    "            url_list.append(_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the JSON file URLs\n",
    "\n",
    "Now walk the list of URLs and for each of the Plan, Provider and Formulary sections, get every URL indicated in those sections. Place the retrieved JSON urls in the corresponding list. Errors are tracked in a separate dictionary. Each is written to separate files: provider, plans, formulary and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "provider_urls = []\n",
    "plan_urls = []\n",
    "formulary_urls = []\n",
    "errors = []\n",
    "\n",
    "for _url in url_list:\n",
    "    try:\n",
    "        response = requests.get(_url)\n",
    "        links = json.loads(response.content)\n",
    "        if 'provider_urls' in links:\n",
    "            for provider_url in links['provider_urls']:\n",
    "                provider_urls.append({'url': provider_url, \n",
    "                                      'status': 'NEW', \n",
    "                                      'parent_url': _url})\n",
    "        if 'formulary_urls' in links:\n",
    "            for formulary_url in links['formulary_urls']:\n",
    "                formulary_urls.append({'url': formulary_url, \n",
    "                                       'status': 'NEW', \n",
    "                                       'parent_url': _url})\n",
    "        if 'plan_urls' in links:\n",
    "            for plan_url in links['plan_urls']:\n",
    "                plan_urls.append({'url': plan_url, \n",
    "                                  'status': 'NEW', \n",
    "                                  'parent_url': _url})\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print \"JSON load failed with this url:\"\n",
    "        print _url\n",
    "        errors.append({'url': _url, 'error': 'JSON load failed', 'message': str(ve)})\n",
    "    except SSLError as se:\n",
    "        print \"SSL Error attempting to negotiate:\"\n",
    "        print _url\n",
    "        errors.append({'url': _url, 'error': 'SSL Error', 'message': str(se)})\n",
    "    except Exception as inst:\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)          # __str__ allows args to be printed directly,\n",
    "        print links\n",
    "        errors.append({'url': _url, 'error': str(type(inst)), 'message': str(inst)})\n",
    "        \n",
    "provider_urlfile = 'provider-urls.txt'\n",
    "plan_urlfile = 'plan-urls.txt'\n",
    "formulary_urlfile = 'formulary-urls.txt'\n",
    "error_file = 'error-urls.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(provider_urlfile, 'w') as prvfile:\n",
    "    for _url in provider_urls:\n",
    "        prvfile.write(\"{0}\\n\".format(json.dumps(_url)))\n",
    "        \n",
    "with open(plan_urlfile, 'w') as planfile:\n",
    "    for _url in plan_urls:\n",
    "        planfile.write(\"{0}\\n\".format(json.dumps(_url)))\n",
    "        \n",
    "with open(formulary_urlfile, 'w') as formfile:\n",
    "    for _url in formulary_urls:\n",
    "        formfile.write(\"{0}\\n\".format(json.dumps(_url)))\n",
    "        \n",
    "with open(error_file, 'w') as errorfile:\n",
    "    for err in errors:\n",
    "        errorfile.write(\"{0}\\n\".format(json.dumps(err)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the JSON file content for all URLs\n",
    "\n",
    "Get all the data pointed to by the provider, plan and formulary urls and store each in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('w210')\n",
    "check_map = {}\n",
    "\n",
    "# reconstitute the dictionary from the file on disk\n",
    "def load_urls(urlfile):\n",
    "    urls = []\n",
    "    with open(urlfile, 'r') as infile:\n",
    "        for line in infile.readlines():\n",
    "            urls.append(json.loads(line.strip())\n",
    "    return urls\n",
    "            \n",
    "for fname in ['provider-urls.txt','plan-urls.txt','formulary-urls.txt']:\n",
    "    urls = load_urls(fname)\n",
    "    for _url in urls:\n",
    "        if _url['status'] == 'NEW':\n",
    "            try:\n",
    "                _url['s3key'] = process_url(_url['url'], 'w210', 'json/')\n",
    "                _url['status'] = 'PROCESSED'\n",
    "            except Exception as ex:\n",
    "                _url['status': 'ERROR']\n",
    "    with open(fname, 'w') as outfile:\n",
    "        with item in url_dict:\n",
    "            outfile.write(\"{0}\\n\".format(json.dumps(_url)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json1985609760036727740\n",
      "0/4193\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in check_map:\n",
    "    print check_map[item]['key']\n",
    "    break\n",
    "    if not item['hash']:\n",
    "        count += 1\n",
    "print \"{0}/{1}\".format(count, len(check_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashes = []\n",
    "for ppf_url in ppf_urls:\n",
    "    hashes.append(str(hash(ppf_url)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71418"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "s3_hashes = []\n",
    "s3 = boto3.resource('s3')\n",
    "for bucket in s3.buckets.all():\n",
    "    for obj in bucket.objects.filter(Prefix='json/'):\n",
    "        s3_hashes.append(re.split('/',obj.key)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2938"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s3_hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5261335633318247304\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for s3_hash in s3_hashes:\n",
    "    if s3_hash not in hashes:\n",
    "        print s3_hash\n",
    "        \n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1001508241897246742\n",
      "-100756317028515332\n",
      "-1012468138642023148\n",
      "-1015077227650838858\n",
      "-1018534659563486333\n",
      "-1027155078582571361\n",
      "-1058517716119348089\n",
      "-1069132213514599781\n",
      "-1087410947984936387\n",
      "-1092631167391773407\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print s3_hashes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985609760036727740\n",
      "59549040364971953\n",
      "9039898652487861151\n",
      "-7179808801752382390\n",
      "-3505892085904121982\n",
      "4833110020626385507\n",
      "-902456558468115489\n",
      "-7107920627704830342\n",
      "4684633488033199749\n",
      "1985609760036727740\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print hashes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
